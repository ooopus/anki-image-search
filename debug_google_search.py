#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•è„šæœ¬ï¼šæŸ¥çœ‹Google Imagesæœç´¢çš„å®žé™…HTMLå“åº”
ç”¨äºŽè¯Šæ–­ä¸ºä»€ä¹ˆæ— æ³•æ‰¾åˆ°å›¾ç‰‡
"""

import sys
import io
import urllib.parse
from pathlib import Path

# Fix Windows console encoding for Chinese characters
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("é”™è¯¯ï¼šéœ€è¦å®‰è£…ä¾èµ–")
    print("è¿è¡Œ: pip install requests beautifulsoup4")
    exit(1)

# Google Imagesæœç´¢URL
GOOGLE_IMAGE_SEARCH_URL = "https://www.google.com/search"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"


def debug_search(query: str):
    """è°ƒè¯•Google Imagesæœç´¢"""
    print(f"\n{'='*60}")
    print(f"æœç´¢å…³é”®è¯: {query}")
    print(f"{'='*60}\n")

    # åˆ›å»ºä¼šè¯
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    # æž„å»ºæœç´¢URL
    params = {
        "q": query,
        "udm": "2",  # Image search mode (unified display mode)
    }
    url = f"{GOOGLE_IMAGE_SEARCH_URL}?{urllib.parse.urlencode(params)}"

    print(f"ðŸ“ è¯·æ±‚URL:\n{url}\n")

    try:
        # å‘é€è¯·æ±‚
        print("ðŸ”„ æ­£åœ¨å‘é€HTTPè¯·æ±‚...")
        response = session.get(url, timeout=10)
        response.raise_for_status()

        print(f"âœ… HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"âœ… å“åº”å¤§å°: {len(response.text)} å­—ç¬¦\n")

        # ä¿å­˜åŽŸå§‹HTML
        output_file = Path("debug_google_response.html")
        output_file.write_text(response.text, encoding="utf-8")
        print(f"ðŸ’¾ åŽŸå§‹HTMLå·²ä¿å­˜åˆ°: {output_file.absolute()}\n")

        # è§£æžHTML
        print("ðŸ” å¼€å§‹è§£æžHTML...\n")
        soup = BeautifulSoup(response.text, "html.parser")

        # æ£€æŸ¥å½“å‰ä»£ç ä½¿ç”¨çš„é€‰æ‹©å™¨
        print("ã€1ã€‘æ£€æŸ¥æ–°çš„è§£æžé€»è¾‘ (img.DS1iW):")

        image_tags = soup.find_all("img", class_="DS1iW")
        print(f"   æ‰¾åˆ° {len(image_tags)} ä¸ª img.DS1iW å…ƒç´ ")

        if image_tags:
            print("   âœ… æ‰¾åˆ°äº†å›¾ç‰‡å…ƒç´ ï¼")

            # æ¨¡æ‹Ÿå®žé™…ä»£ç çš„è§£æžé€»è¾‘
            results = []
            for img_tag in image_tags[:5]:
                img_url = img_tag.get("src")

                if not img_url or img_url.startswith("data:") or img_url.startswith("/images/branding"):
                    continue

                thumbnail_url = img_url
                title = img_tag.get("alt", "")

                # Try to find parent link
                parent_link = img_tag.find_parent("a")
                source_url = ""
                if parent_link:
                    href = parent_link.get("href", "")
                    if href and "imgurl=" in href:
                        parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                        if "imgurl" in parsed:
                            source_url = parsed["imgurl"][0]
                    else:
                        source_url = href

                final_url = source_url if source_url and source_url.startswith("http") else img_url

                results.append({
                    "url": final_url,
                    "thumbnail": thumbnail_url,
                    "title": title or "Image",
                    "source": source_url,
                })

            print(f"   âœ… æˆåŠŸè§£æž {len(results)} å¼ å›¾ç‰‡\n")

            for i, result in enumerate(results[:3], 1):
                print(f"   å›¾ç‰‡ #{i}:")
                print(f"   - URL: {result['url'][:80]}")
                print(f"   - ç¼©ç•¥å›¾: {result['thumbnail'][:80]}")
                print(f"   - æ ‡é¢˜: {result['title']}")
                print(f"   - æ¥æº: {result['source'][:80] if result['source'] else '(æ— )'}")
                print()
        else:
            print("   âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å…ƒç´ ï¼")

        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
        print("\nã€2ã€‘å°è¯•å…¶ä»–å¸¸è§çš„å›¾ç‰‡é€‰æ‹©å™¨:")

        selectors = [
            ("æ‰€æœ‰ img æ ‡ç­¾", "img"),
            ("æ‰€æœ‰ a æ ‡ç­¾", "a"),
            ("classåŒ…å«'rg_i'çš„img", "img.rg_i"),
            ("classåŒ…å«'Q4LuWd'çš„img", "img.Q4LuWd"),
            ("data-srcå±žæ€§çš„img", "img[data-src]"),
        ]

        for desc, selector in selectors:
            elements = soup.select(selector)
            print(f"   {desc}: {len(elements)} ä¸ª")

        # åˆ†æžæ‰€æœ‰imgæ ‡ç­¾
        print("\nã€3ã€‘åˆ†æžæ‰€æœ‰imgæ ‡ç­¾çš„è¯¦ç»†ä¿¡æ¯:")
        all_images = soup.find_all("img")
        print(f"   æ€»å…±æ‰¾åˆ° {len(all_images)} ä¸ªimgæ ‡ç­¾\n")

        if all_images:
            for i, img in enumerate(all_images[:5], 1):
                print(f"   å›¾ç‰‡ #{i}:")
                print(f"   - class: {img.get('class', [])}")
                print(f"   - src: {img.get('src', 'N/A')[:80]}")
                print(f"   - data-src: {img.get('data-src', 'N/A')[:80]}")
                print(f"   - data-iurl: {img.get('data-iurl', 'N/A')[:80]}")
                print(f"   - alt: {img.get('alt', 'N/A')[:50]}")
                print()

        # æŸ¥æ‰¾æ‰€æœ‰divçš„classå±žæ€§
        print("\nã€4ã€‘åˆ†æžæ‰€æœ‰divçš„classå±žæ€§ï¼ˆå‰20ä¸ªï¼‰:")
        all_divs = soup.find_all("div", class_=True)
        unique_classes = set()
        for div in all_divs[:100]:
            classes = div.get("class", [])
            if isinstance(classes, list):
                unique_classes.update(classes)

        sorted_classes = sorted(unique_classes)
        print(f"   æ‰¾åˆ° {len(sorted_classes)} ä¸ªå”¯ä¸€çš„classåç§°")
        print("   å‰20ä¸ªclassåç§°:")
        for cls in sorted_classes[:20]:
            print(f"   - {cls}")

        # æ£€æŸ¥æ˜¯å¦æœ‰CAPTCHAæˆ–é”™è¯¯é¡µé¢
        print("\nã€5ã€‘æ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬è™«/CAPTCHA:")
        captcha_indicators = [
            "captcha",
            "recaptcha",
            "automated",
            "unusual traffic",
            "robot",
        ]

        page_text = response.text.lower()
        found_indicators = [
            indicator for indicator in captcha_indicators if indicator in page_text
        ]

        if found_indicators:
            print(f"   âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„åçˆ¬è™«æœºåˆ¶:")
            for indicator in found_indicators:
                print(f"   - '{indicator}'")
        else:
            print("   âœ… æœªæ£€æµ‹åˆ°æ˜Žæ˜¾çš„åçˆ¬è™«æœºåˆ¶")

        print(f"\n{'='*60}")
        print("è°ƒè¯•å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡ºå’Œä¿å­˜çš„HTMLæ–‡ä»¶")
        print(f"{'='*60}\n")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # ä½¿ç”¨ç®€å•çš„æµ‹è¯•å…³é”®è¯
    test_query = "cat"
    debug_search(test_query)

    print("\næç¤ºï¼šä½ å¯ä»¥ä¿®æ”¹è„šæœ¬ä¸­çš„ test_query å˜é‡æ¥æµ‹è¯•ä¸åŒçš„æœç´¢è¯")
    print("      æˆ–è€…ç”¨å‘½ä»¤è¡Œå‚æ•°: python debug_google_search.py \"your search term\"")

    import sys

    if len(sys.argv) > 1:
        custom_query = " ".join(sys.argv[1:])
        debug_search(custom_query)
